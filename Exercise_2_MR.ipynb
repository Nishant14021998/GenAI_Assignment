{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install python-dotenv pydantic langchain langchain-groq langgraph --quiet"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zPSe5dkMhzUK",
        "outputId": "92ae6f08-82dd-4502-ca33-73568c866e3d"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/142.6 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m142.6/142.6 kB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/109.6 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m109.6/109.6 kB\u001b[0m \u001b[31m7.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/44.7 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.7/44.7 kB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kmsKQjcahcp8",
        "outputId": "e42721ff-eff5-4fbe-acd8-09f6cbf5270d"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "False"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ],
      "source": [
        "import os, getpass\n",
        "from dotenv import load_dotenv\n",
        "load_dotenv(\".env\", override=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mbzt5g6ehcp9",
        "outputId": "1216f0cf-e1a5-4a5b-9927-0d60770044bb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Enter your GROQ API Key: ··········\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import json\n",
        "import re\n",
        "from getpass import getpass\n",
        "from google.colab import userdata\n",
        "from dotenv import load_dotenv\n",
        "\n",
        "load_dotenv(\".env\", override=True)\n",
        "\n",
        "if \"GROQ_API_KEY\" not in os.environ:\n",
        "    try:\n",
        "        os.environ[\"GROQ_API_KEY\"] = userdata.get('GROQ_API_KEY')\n",
        "    except Exception:\n",
        "        os.environ[\"GROQ_API_KEY\"] = getpass(\"Enter your GROQ API Key: \")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "c3-7ZBe9hcp_"
      },
      "outputs": [],
      "source": [
        "from pydantic import BaseModel, Field\n",
        "\n",
        "class WeatherData(BaseModel):\n",
        "    \"\"\"Represents weather information.\"\"\"\n",
        "\n",
        "    temperature: float = Field(description=\"Temperature in Fahrenheit\")\n",
        "    wind_direction: str = Field(description=\"Wind direction (abbreviated)\")\n",
        "    wind_speed: float = Field(description=\"Wind speed in km/h\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "nBNwLeVThcqA"
      },
      "outputs": [],
      "source": [
        "from typing import Literal\n",
        "from pydantic import BaseModel, Field  # Import for data validation and type hinting\n",
        "from langchain_core.tools import tool  # Import for creating LangChain tools\n",
        "from langchain_groq import ChatGroq  # Import for using the Groq language model\n",
        "from langgraph.graph import MessagesState  # Import for managing chat messages in a graph\n",
        "\n",
        "# Define a Pydantic model to represent weather data\n",
        "class WeatherResponse(BaseModel):\n",
        "    \"\"\"Represents weather information.\"\"\"\n",
        "    temperature: float = Field(description=\"Temperature in Fahrenheit\")\n",
        "    wind_direction: str = Field(description=\"Wind direction (abbreviated)\")\n",
        "    wind_speed: float = Field(description=\"Wind speed in km/h\")\n",
        "\n",
        "# Define a class to represent the agent's state in the conversation\n",
        "class AgentState(MessagesState):\n",
        "    \"\"\"\n",
        "    Stores the state of the agent in the conversation.\n",
        "    Inherits the 'messages' attribute from MessagesState to hold the chat history.\n",
        "    \"\"\"\n",
        "    # Attribute to store the final structured response from the agent\n",
        "    final_response: WeatherResponse\n",
        "\n",
        "# Define a tool function to retrieve weather information\n",
        "@tool\n",
        "def get_weather(city: Literal['nyc', 'sf']) -> str:\n",
        "    \"\"\"\n",
        "    Retrieves weather information for a specified city.\n",
        "    Args:\n",
        "        city: The city to get weather information for (either 'nyc' or 'sf').\n",
        "    Returns:\n",
        "        A string containing the weather information for the specified city.\n",
        "    \"\"\"\n",
        "    if city == 'nyc':\n",
        "        return 'It is cloudy in NYC, with 5 mph winds in the North-East direction and a temperature of 70 degrees'\n",
        "    elif city == 'sf':\n",
        "        return \"It is 75 degrees and sunny in SF, with 3 mph winds in the South-East direction\"  # Corrected \"ph\" to \"mph\"\n",
        "    else:\n",
        "        raise AssertionError(\"Unknown city\")\n",
        "\n",
        "# Create a list of tools to be used by the agent\n",
        "tools = [get_weather]\n",
        "\n",
        "# Initialize the Groq language model\n",
        "model = ChatGroq(model='llama3-8b-8192')\n",
        "\n",
        "# Bind the tools to the model\n",
        "model_with_tools = model.bind_tools(tools)\n",
        "\n",
        "# Configure the model to produce structured output using the WeatherData model\n",
        "model_with_structured_output = model.with_structured_output(WeatherResponse)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K0H6zTMQhcqA"
      },
      "source": [
        "## Option 1 single LLM option"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VFIqCkzRhcqB"
      },
      "source": [
        "### Define Graph"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "8k0IQLLqhcqB"
      },
      "outputs": [],
      "source": [
        "from langgraph.graph import StateGraph, END\n",
        "from langgraph.prebuilt import ToolNode\n",
        "\n",
        "tools = [get_weather, WeatherResponse]\n",
        "\n",
        "#Force the model to use tools by passing tool_choice = 'any'\n",
        "\n",
        "model_with_response_tool = model.bind_tools(tools,\n",
        "                                            tool_choice='any')\n",
        "\n",
        "# Define the function that calls the model\n",
        "def call_model(state: AgentState):\n",
        "    response = model_with_response_tool.invoke(state['messages'])\n",
        "\n",
        "    # We return a list, because this will get added to the existing list\n",
        "    return {'messages': [response]}\n",
        "\n",
        "# Define the function that reponds to user\n",
        "def respond(state: AgentState):\n",
        "    # Construct the final answer from the arguments of the last tool call\n",
        "    weather_tool_call = state['messages'][-1].tool_calls[0]\n",
        "    response = WeatherResponse(**weather_tool_call['args'])\n",
        "\n",
        "    #since we are using tool calling to return structured output\n",
        "    # we need to add a tool message corresponding to the WeatherResponse tool call,\n",
        "    # This is due to LLM providers' requirement that AI messages with tool calls\n",
        "    # need to be follwed by a tool message for each tool call\n",
        "\n",
        "    tool_message = {\n",
        "        'type' : 'tool',\n",
        "        'content': 'Here is your structured response',\n",
        "        'tool_call_id' : weather_tool_call['id'],\n",
        "    }\n",
        "\n",
        "    # We return the final answer\n",
        "    return {'final_response': response,\n",
        "            'messages' : [tool_message]}\n",
        "\n",
        "def should_continue(state: AgentState):\n",
        "    message = state['messages']\n",
        "    last_message = message[-1]\n",
        "    # If there is only tool call and it is the response tool call we respond to the user\n",
        "\n",
        "    if (\n",
        "        len(last_message.tool_calls) == 1\n",
        "        and\n",
        "        last_message.tool_calls[0]['name'] == 'WeatherResponse'\n",
        "    ):\n",
        "        return 'respond'\n",
        "    # Otherwise we will use the tool node again\n",
        "    else:\n",
        "        return 'continue'\n",
        "\n",
        "# Define a new graph\n",
        "workflow = StateGraph(AgentState)\n",
        "\n",
        "# Define the two nodes we will cycle between\n",
        "workflow.add_node(\"agent\", call_model)\n",
        "workflow.add_node(\"respond\",respond)\n",
        "workflow.add_node('tools', ToolNode(tools))\n",
        "\n",
        "# Set the entrypoint as 'agent'\n",
        "# This means that this node is the first one called\n",
        "workflow.set_entry_point('agent')\n",
        "\n",
        "# We now add a conditional edge\n",
        "workflow.add_conditional_edges(\n",
        "    'agent',\n",
        "    should_continue,\n",
        "    {\n",
        "        'continue': 'tools',\n",
        "        'respond' : 'respond'\n",
        "    }\n",
        ")\n",
        "\n",
        "workflow.add_edge('tools','agent')\n",
        "workflow.add_edge('respond',END)\n",
        "graph = workflow.compile()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vxxpK_cjhcqC"
      },
      "source": [
        "## Usage"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "EYhs01RAhcqC"
      },
      "outputs": [],
      "source": [
        "answer = graph.invoke(input={\n",
        "                            'messages': [('human',\n",
        "                                        \"what's the weather in SF\")]\n",
        "                            })['final_response']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SsBbrrr6hcqC",
        "outputId": "d08147e6-f63f-480f-db7c-5b196bb8d97e"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "WeatherResponse(temperature=75.0, wind_direction='S-E', wind_speed=3.0)"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ],
      "source": [
        "answer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Se0mNTvNhcqD"
      },
      "source": [
        "## Option 2: 2 LLMs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g9cu0vYqhcqD"
      },
      "source": [
        "### Define Graph"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "opjrIBi1hcqD"
      },
      "outputs": [],
      "source": [
        "from langgraph.graph import StateGraph, END\n",
        "from langgraph.prebuilt import ToolNode\n",
        "from langchain_core.messages import HumanMessage\n",
        "\n",
        "\n",
        "# Define the function that calls the model\n",
        "def call_model(state: AgentState):\n",
        "    response = model_with_tools.invoke(state[\"messages\"])\n",
        "    # We return a list, because this will get added to the existing list\n",
        "    return {\"messages\": [response]}\n",
        "\n",
        "\n",
        "# Define the function that responds to the user\n",
        "def respond(state: AgentState):\n",
        "    # We call the model with structured output in order to return the same format to the user every time\n",
        "    # state['messages'][-2] is the last ToolMessage in the convo, which we convert to a HumanMessage for the model to use\n",
        "    # We could also pass the entire chat history, but this saves tokens since all we care to structure is the output of the tool\n",
        "    response = model_with_structured_output.invoke(\n",
        "        [HumanMessage(content=state[\"messages\"][-2].content)]\n",
        "    )\n",
        "    # We return the final answer\n",
        "    return {\"final_response\": response}\n",
        "\n",
        "\n",
        "# Define the function that determines whether to continue or not\n",
        "def should_continue(state: AgentState):\n",
        "    messages = state[\"messages\"]\n",
        "    last_message = messages[-1]\n",
        "    # If there is no function call, then we respond to the user\n",
        "    if not last_message.tool_calls:\n",
        "        return \"respond\"\n",
        "    # Otherwise if there is, we continue\n",
        "    else:\n",
        "        return \"continue\"\n",
        "\n",
        "\n",
        "# Define a new graph\n",
        "workflow = StateGraph(AgentState)\n",
        "\n",
        "# Define the two nodes we will cycle between\n",
        "workflow.add_node(\"agent\", call_model)\n",
        "workflow.add_node(\"respond\", respond)\n",
        "workflow.add_node(\"tools\", ToolNode(tools))\n",
        "\n",
        "# Set the entrypoint as `agent`\n",
        "# This means that this node is the first one called\n",
        "workflow.set_entry_point(\"agent\")\n",
        "\n",
        "# We now add a conditional edge\n",
        "workflow.add_conditional_edges(\n",
        "    \"agent\",\n",
        "    should_continue,\n",
        "    {\n",
        "        \"continue\": \"tools\",\n",
        "        \"respond\": \"respond\",\n",
        "    },\n",
        ")\n",
        "\n",
        "workflow.add_edge(\"tools\", \"agent\")\n",
        "workflow.add_edge(\"respond\", END)\n",
        "graph = workflow.compile()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "hF3118kphcqD"
      },
      "outputs": [],
      "source": [
        "answer = graph.invoke(input={\"messages\": [(\"human\", \"what's the weather in SF?\")]})[\n",
        "    \"final_response\"\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bFGkw8T4hcqD",
        "outputId": "9efe4131-c754-42d6-9bdf-62b7754668ed"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "WeatherResponse(temperature=75.0, wind_direction='South-East', wind_speed=3.0)"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ],
      "source": [
        "answer"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "genai_course_env",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.1"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}